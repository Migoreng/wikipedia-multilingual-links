#!/usr/bin/env python3
"""
Wikipedia å¤šè¨€èªå¯¾å¿œè¡¨ä½œæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- ä»»æ„ã®è¨€èªãƒšã‚¢ã«å¯¾å¿œ
- ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½
- 3è¨€èªä»¥ä¸Šã®çµ„ã¿åˆã‚ã›ã«ã‚‚å¯¾å¿œ
"""

import os
import sys
import gzip
import csv
import argparse
import requests
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm


# ========================================
# è¨€èªã‚³ãƒ¼ãƒ‰ â†’ è¡¨ç¤ºåã®ãƒãƒƒãƒ”ãƒ³ã‚°
# ========================================

LANGUAGE_NAMES = {
    'en': 'è‹±èª',
    'ja': 'æ—¥æœ¬èª',
    'de': 'ãƒ‰ã‚¤ãƒ„èª',
    'fr': 'ãƒ•ãƒ©ãƒ³ã‚¹èª',
    'es': 'ã‚¹ãƒšã‚¤ãƒ³èª',
    'it': 'ã‚¤ã‚¿ãƒªã‚¢èª',
    'pt': 'ãƒãƒ«ãƒˆã‚¬ãƒ«èª',
    'ru': 'ãƒ­ã‚·ã‚¢èª',
    'zh': 'ä¸­å›½èª',
    'ko': 'éŸ“å›½èª',
    'ar': 'ã‚¢ãƒ©ãƒ“ã‚¢èª',
    'la': 'ãƒ©ãƒ†ãƒ³èª',
    'nl': 'ã‚ªãƒ©ãƒ³ãƒ€èª',
    'pl': 'ãƒãƒ¼ãƒ©ãƒ³ãƒ‰èª',
    'sv': 'ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³èª',
    'he': 'ãƒ˜ãƒ–ãƒ©ã‚¤èª',
    'tr': 'ãƒˆãƒ«ã‚³èª',
    'cs': 'ãƒã‚§ã‚³èª',
    'el': 'ã‚®ãƒªã‚·ãƒ£èª',
    'fi': 'ãƒ•ã‚£ãƒ³ãƒ©ãƒ³ãƒ‰èª',
}


# ========================================
# 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
# ========================================

def download_file(url, output_path, chunk_size=8192):
    """
    HTTP GETã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆé€²æ—ãƒãƒ¼ä»˜ãã€ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ å¯¾å¿œï¼‰
    """
    output_path = Path(output_path)
    
    # ã™ã§ã«å®Œå…¨ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
    headers = {}
    if output_path.exists():
        existing_size = output_path.stat().st_size
        headers['Range'] = f'bytes={existing_size}-'
        print(f"ğŸ“‚ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {output_path.name} ({existing_size:,} bytes)")
        print(f"   â†’ é€”ä¸­ã‹ã‚‰å†é–‹ã—ã¾ã™")
    
    try:
        response = requests.get(url, headers=headers, stream=True, timeout=30)
    except requests.exceptions.RequestException as e:
        print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # 206 Partial Content ã¾ãŸã¯ 200 OK
    if response.status_code == 416:  # Range Not Satisfiable = ã™ã§ã«å®Œäº†
        print(f"âœ… {output_path.name} ã¯ã™ã§ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™")
        return True
    
    if response.status_code not in (200, 206):
        print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼ {response.status_code}: {url}")
        return False
    
    total_size = int(response.headers.get('content-length', 0))
    mode = 'ab' if response.status_code == 206 else 'wb'
    
    print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {output_path.name}")
    
    with open(output_path, mode) as f, tqdm(
        total=total_size,
        initial=output_path.stat().st_size if mode == 'ab' else 0,
        unit='B',
        unit_scale=True,
        unit_divisor=1024,
    ) as pbar:
        for chunk in response.iter_content(chunk_size=chunk_size):
            if chunk:
                f.write(chunk)
                pbar.update(len(chunk))
    
    print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {output_path.name}\n")
    return True


def download_wikipedia_dumps(lang):
    """
    Wikipedia dumps (langlinks + page) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    base_url = f"https://dumps.wikimedia.org/{lang}wiki/latest/"
    files = [
        f"{lang}wiki-latest-langlinks.sql.gz",
        f"{lang}wiki-latest-page.sql.gz"
    ]
    
    lang_name = LANGUAGE_NAMES.get(lang, lang.upper())
    print(f"{'='*60}")
    print(f"Wikipedia {lang_name} ({lang}) ãƒ€ãƒ³ãƒ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    print(f"{'='*60}\n")
    
    for filename in files:
        url = base_url + filename
        success = download_file(url, filename)
        if not success:
            print(f"âš ï¸  {filename} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
    
    return True


# ========================================
# 2. ãƒ‘ãƒ¼ã‚¹æ©Ÿèƒ½
# ========================================

def parse_sql_insert(line):
    """
    INSERT INTOæ–‡ã‹ã‚‰å€¤éƒ¨åˆ†ã‚’æŠ½å‡º
    """
    if not line.startswith('INSERT INTO'):
        return []
    
    start = line.find('VALUES') + 6
    if start == 5:
        return []
    
    values_str = line[start:].rstrip(';\n')
    rows = []
    depth = 0
    current = []
    field = ''
    in_quote = False
    escape_next = False
    
    for char in values_str:
        if escape_next:
            field += char
            escape_next = False
            continue
        
        if char == '\\':
            escape_next = True
            field += char
            continue
        
        if char == "'" and not escape_next:
            in_quote = not in_quote
            continue
        
        if in_quote:
            field += char
            continue
        
        if char == '(':
            depth += 1
            if depth == 1:
                current = []
            continue
        
        if char == ')':
            depth -= 1
            if depth == 0:
                if field:
                    current.append(field)
                    field = ''
                if current:
                    rows.append(tuple(current))
            continue
        
        if char == ',' and depth == 1:
            current.append(field)
            field = ''
            continue
        
        if depth == 1:
            field += char
    
    return rows


def parse_wikipedia_dump(source_lang, target_lang):
    """
    Wikipediaãƒ€ãƒ³ãƒ—ã‹ã‚‰æŒ‡å®šè¨€èªã¸ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    """
    source_name = LANGUAGE_NAMES.get(source_lang, source_lang.upper())
    target_name = LANGUAGE_NAMES.get(target_lang, target_lang.upper())
    
    print(f"\n{'='*60}")
    print(f"Wikipedia {source_name} ({source_lang}) â†’ {target_name} ({target_lang}) ãƒ‘ãƒ¼ã‚¹é–‹å§‹")
    print(f"{'='*60}\n")
    
    langlinks_file = f"{source_lang}wiki-latest-langlinks.sql.gz"
    page_file = f"{source_lang}wiki-latest-page.sql.gz"
    output_file = f"{source_lang}_{target_lang}_all.csv"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not Path(langlinks_file).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {langlinks_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    if not Path(page_file).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {page_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: langlinks ã‹ã‚‰å¯¾è±¡è¨€èªãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    print(f"ã‚¹ãƒ†ãƒƒãƒ—1: {langlinks_file} èª­ã¿è¾¼ã¿ä¸­...")
    page_to_target = {}
    
    with gzip.open(langlinks_file, 'rt', encoding='utf-8', errors='ignore') as f:
        for line in tqdm(f, desc="langlinkså‡¦ç†"):
            rows = parse_sql_insert(line)
            for row in rows:
                if len(row) >= 3 and row[1] == target_lang:
                    page_id = row[0]
                    target_title = row[2]
                    page_to_target[page_id] = target_title
    
    print(f"  â†’ {len(page_to_target):,} ä»¶ã®{target_name}ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º\n")
    
    if len(page_to_target) == 0:
        print(f"âš ï¸  è­¦å‘Š: {target_name}ã¸ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: page ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    print(f"ã‚¹ãƒ†ãƒƒãƒ—2: {page_file} èª­ã¿è¾¼ã¿ä¸­...")
    results = []
    
    with gzip.open(page_file, 'rt', encoding='utf-8', errors='ignore') as f:
        for line in tqdm(f, desc="pageå‡¦ç†"):
            rows = parse_sql_insert(line)
            for row in rows:
                if len(row) >= 3:
                    page_id = row[0]
                    namespace = row[1]
                    title = row[2]
                    
                    if namespace == '0' and page_id in page_to_target:
                        results.append({
                            'page_id': page_id,
                            source_lang: title,
                            target_lang: page_to_target[page_id]
                        })
    
    print(f"  â†’ {len(results):,} ä»¶ã®ãƒãƒƒãƒãƒ³ã‚°å®Œäº†\n")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: CSVå‡ºåŠ›
    print(f"ã‚¹ãƒ†ãƒƒãƒ—3: {output_file} ã«ä¿å­˜ä¸­...")
    
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        fieldnames = ['page_id', source_name, target_name]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for item in results:
            writer.writerow({
                'page_id': item['page_id'],
                source_name: item[source_lang],
                target_name: item[target_lang]
            })
    
    print(f"âœ… å®Œäº†: {output_file} ({len(results):,} ä»¶)\n")
    return output_file


# ========================================
# 3. ãƒãƒ¼ã‚¸æ©Ÿèƒ½ï¼ˆNè¨€èªå¯¾å¿œï¼‰
# ========================================

def merge_languages(lang_files, bridge_lang, output_file):
    """
    è¤‡æ•°ã®è¨€èªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ–ãƒªãƒƒã‚¸è¨€èªã§ãƒãƒ¼ã‚¸
    
    Args:
        lang_files: [(lang_code, csv_file), ...] ã®ãƒªã‚¹ãƒˆ
        bridge_lang: ãƒ–ãƒªãƒƒã‚¸è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: 'la', 'en'ï¼‰
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    bridge_name = LANGUAGE_NAMES.get(bridge_lang, bridge_lang.upper())
    
    print(f"{'='*60}")
    print(f"å¤šè¨€èªãƒãƒ¼ã‚¸ï¼ˆãƒ–ãƒªãƒƒã‚¸: {bridge_name}ï¼‰")
    print(f"{'='*60}\n")
    
    # å„è¨€èªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
    all_data = {}
    lang_names = []
    
    for lang_code, csv_file in lang_files:
        lang_name = LANGUAGE_NAMES.get(lang_code, lang_code.upper())
        lang_names.append((lang_code, lang_name))
        
        print(f"ã‚¹ãƒ†ãƒƒãƒ—: {csv_file} èª­ã¿è¾¼ã¿ä¸­...")
        
        if not Path(csv_file).exists():
            print(f"âš ï¸  è­¦å‘Š: {csv_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        
        with open(csv_file, encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # ãƒ–ãƒªãƒƒã‚¸è¨€èªã®å€¤ã‚’å–å¾—
                bridge_value = None
                lang_value = None
                
                for key, value in row.items():
                    if bridge_name in key or bridge_lang in key.lower():
                        bridge_value = value.strip()
                    elif lang_name in key or lang_code in key.lower():
                        lang_value = value.strip()
                
                if bridge_value and lang_value:
                    if bridge_value not in all_data:
                        all_data[bridge_value] = {}
                    
                    if lang_code not in all_data[bridge_value]:
                        all_data[bridge_value][lang_code] = []
                    
                    if lang_value not in all_data[bridge_value][lang_code]:
                        all_data[bridge_value][lang_code].append(lang_value)
        
        total_entries = sum(len(v.get(lang_code, [])) for v in all_data.values())
        print(f"  â†’ {total_entries:,} ä»¶èª­ã¿è¾¼ã¿\n")
    
    # ãƒãƒ¼ã‚¸çµæœã‚’æ§‹ç¯‰
    print("ã‚¹ãƒ†ãƒƒãƒ—: ãƒãƒ¼ã‚¸å‡¦ç†ä¸­...")
    merged = []
    
    for bridge_value, lang_dict in all_data.items():
        # ã™ã¹ã¦ã®è¨€èªã§ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‚‚ã®ã®ã¿
        if len(lang_dict) == len(lang_files):
            # å„è¨€èªã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
            from itertools import product
            combinations = product(*[lang_dict[lc] for lc, _ in lang_names])
            
            for combo in combinations:
                row = {LANGUAGE_NAMES.get(lc, lc): val for (lc, _), val in zip(lang_names, combo)}
                row[bridge_name] = bridge_value
                merged.append(row)
    
    print(f"  â†’ {len(merged):,} ä»¶ã®ãƒãƒƒãƒãƒ³ã‚°å®Œäº†\n")
    
    # é‡è¤‡å‰Šé™¤
    print("ã‚¹ãƒ†ãƒƒãƒ—: é‡è¤‡å‰Šé™¤ä¸­...")
    seen = set()
    unique_merged = []
    
    for item in merged:
        key = tuple(item.values())
        if key not in seen:
            seen.add(key)
            unique_merged.append(item)
    
    print(f"  â†’ é‡è¤‡å‰Šé™¤å¾Œ: {len(unique_merged):,} ä»¶\n")
    
    # ã‚½ãƒ¼ãƒˆ & ä¿å­˜
    unique_merged.sort(key=lambda x: x[bridge_name])
    
    print(f"ã‚¹ãƒ†ãƒƒãƒ—: {output_file} ã«ä¿å­˜ä¸­...")
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’æ±ºå®šï¼ˆãƒ–ãƒªãƒƒã‚¸è¨€èªã‚’æœ€å¾Œã«ï¼‰
    fieldnames = [LANGUAGE_NAMES.get(lc, lc) for lc, _ in lang_names]
    if bridge_name not in fieldnames:
        fieldnames.append(bridge_name)
    else:
        fieldnames.remove(bridge_name)
        fieldnames.append(bridge_name)
    
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(unique_merged)
    
    print(f"âœ… å®Œäº†: {output_file} ({len(unique_merged):,} ä»¶)\n")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    if unique_merged:
        print("çµæœã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
        print("-" * 100)
        print(" | ".join(f"{fn:30}" for fn in fieldnames))
        print("-" * 100)
        for item in unique_merged[:10]:
            print(" | ".join(f"{item.get(fn, ''):30}" for fn in fieldnames))
    
    return output_file


# ========================================
# 4. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================

def main():
    parser = argparse.ArgumentParser(
        description='Wikipedia å¤šè¨€èªå¯¾å¿œè¡¨ä½œæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # æ—¥æœ¬èª-è‹±èª-ãƒ©ãƒ†ãƒ³èª
  python build_multilingual_wikipedia.py ja en --bridge la
  
  # ãƒ‰ã‚¤ãƒ„èª-è‹±èª-ãƒ©ãƒ†ãƒ³èª
  python build_multilingual_wikipedia.py de en --bridge la
  
  # ã‚¹ãƒšã‚¤ãƒ³èª-ãƒ•ãƒ©ãƒ³ã‚¹èª-ã‚¤ã‚¿ãƒªã‚¢èª-ãƒ©ãƒ†ãƒ³èª
  python build_multilingual_wikipedia.py es fr it --bridge la
  
  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã¿ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã™ã§ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆï¼‰
  python build_multilingual_wikipedia.py ja en --bridge la --skip-download
  
å¯¾å¿œè¨€èªã‚³ãƒ¼ãƒ‰:
  en (è‹±èª), ja (æ—¥æœ¬èª), de (ãƒ‰ã‚¤ãƒ„èª), fr (ãƒ•ãƒ©ãƒ³ã‚¹èª), 
  es (ã‚¹ãƒšã‚¤ãƒ³èª), it (ã‚¤ã‚¿ãƒªã‚¢èª), pt (ãƒãƒ«ãƒˆã‚¬ãƒ«èª), 
  ru (ãƒ­ã‚·ã‚¢èª), zh (ä¸­å›½èª), ko (éŸ“å›½èª), ar (ã‚¢ãƒ©ãƒ“ã‚¢èª),
  la (ãƒ©ãƒ†ãƒ³èª), nl (ã‚ªãƒ©ãƒ³ãƒ€èª), pl (ãƒãƒ¼ãƒ©ãƒ³ãƒ‰èª), ãªã©
        """
    )
    
    parser.add_argument(
        'languages',
        nargs='+',
        help='å‡¦ç†ã™ã‚‹è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: ja en deï¼‰'
    )
    
    parser.add_argument(
        '--bridge',
        default='la',
        help='ãƒ–ãƒªãƒƒã‚¸è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: la [ãƒ©ãƒ†ãƒ³èª]ï¼‰'
    )
    
    parser.add_argument(
        '--output',
        help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è‡ªå‹•ç”Ÿæˆï¼‰'
    )
    
    parser.add_argument(
        '--skip-download',
        action='store_true',
        help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )
    
    parser.add_argument(
        '--skip-parse',
        action='store_true',
        help='ãƒ‘ãƒ¼ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã§ã«ã‚ã‚‹å ´åˆï¼‰'
    )
    
    args = parser.parse_args()
    
    languages = args.languages
    bridge_lang = args.bridge
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®è‡ªå‹•ç”Ÿæˆ
    if args.output:
        output_file = args.output
    else:
        lang_part = '_'.join(languages)
        output_file = f"{lang_part}_{bridge_lang}_all.csv"
    
    print("\n" + "="*60)
    print("Wikipedia å¤šè¨€èªå¯¾å¿œè¡¨ ä½œæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("="*60)
    print(f"å¯¾è±¡è¨€èª: {', '.join([LANGUAGE_NAMES.get(l, l) for l in languages])}")
    print(f"ãƒ–ãƒªãƒƒã‚¸è¨€èª: {LANGUAGE_NAMES.get(bridge_lang, bridge_lang)}")
    print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
    print("="*60 + "\n")
    
    # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if not args.skip_download:
        print("ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n")
        for lang in languages:
            success = download_wikipedia_dumps(lang)
            if not success:
                print(f"âš ï¸  {lang} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç¶šè¡Œã—ã¾ã™...")
    else:
        print("ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - ã‚¹ã‚­ãƒƒãƒ—\n")
    
    # 2. ãƒ‘ãƒ¼ã‚¹
    if not args.skip_parse:
        print("\nã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘ãƒ‘ãƒ¼ã‚¹\n")
        for lang in languages:
            result = parse_wikipedia_dump(lang, bridge_lang)
            if not result:
                print(f"âš ï¸  {lang} ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç¶šè¡Œã—ã¾ã™...")
    else:
        print("\nã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘ãƒ‘ãƒ¼ã‚¹ - ã‚¹ã‚­ãƒƒãƒ—\n")
    
    # 3. ãƒãƒ¼ã‚¸
    print("\nã€ãƒ•ã‚§ãƒ¼ã‚º3ã€‘ãƒãƒ¼ã‚¸\n")
    lang_files = [(lang, f"{lang}_{bridge_lang}_all.csv") for lang in languages]
    merge_languages(lang_files, bridge_lang, output_file)
    
    print("\n" + "="*60)
    print("ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("="*60)
    print(f"\næœ€çµ‚æˆæœç‰©: {output_file}")


if __name__ == "__main__":
    main()

